{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFV94MR8yVYb",
        "outputId": "815c065d-a4fc-4965-c099-ab0c70f8e5ad"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import hstack, csr_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import pandas as pd\n",
        "\n",
        "path = '/content/drive/My Drive/DD2417/question_pairs_cleaned.csv'\n",
        "# 1. Load & clean data (assume already cleaned on disk)\n",
        "df = pd.read_csv(\n",
        "    path,\n",
        "    engine=\"python\",\n",
        "    quotechar='\"',\n",
        "    sep=\",\",\n",
        "    names=['id','qid1','qid2','question1','question2','is_duplicate'],\n",
        "    header=0,\n",
        "    on_bad_lines='skip'\n",
        ")\n",
        "df = df.dropna(subset=['question1','question2','is_duplicate']).copy()\n",
        "df['is_duplicate'] = df['is_duplicate'].astype(int)\n",
        "\n",
        "# 2. Build & fit TF-IDF on all questions\n",
        "all_q = pd.concat([df['question1'], df['question2']])\n",
        "tfidf = TfidfVectorizer(lowercase=True, stop_words='english',\n",
        "                        max_features=10000, ngram_range=(1,2))\n",
        "tfidf.fit(all_q)\n",
        "\n",
        "# 3. Transform each side into sparse TF-IDF\n",
        "X1 = tfidf.transform(df['question1'])\n",
        "X2 = tfidf.transform(df['question2'])\n",
        "\n",
        "# 4. Row-wise dot-product as a similarity feature\n",
        "#    This is effectively the numerator of cosine_similarity,\n",
        "#    and stays sparse, size = (n_samples,1)\n",
        "sim = X1.multiply(X2).sum(axis=1)\n",
        "# convert the column vector into a CSR sparse matrix\n",
        "sim_sparse = csr_matrix(sim)\n",
        "\n",
        "# 5. Stack everything into one big sparse feature matrix\n",
        "#    [ TF-IDF(q1) | TF-IDF(q2) | similarity ]\n",
        "X = hstack([X1, X2, sim_sparse])\n",
        "\n",
        "# 6. Split into train & test\n",
        "y = df['is_duplicate'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# 7. Train a LogisticRegression on the sparse matrix\n",
        "clf = LogisticRegression(\n",
        "    solver='saga',        # sparse-friendly solver\n",
        "    max_iter=500,\n",
        "    class_weight='balanced',\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# 8. Evaluate\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vs_ptcuOy0S_",
        "outputId": "fd6cdd34-4a48-4143-d807-a73ff1b122f5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "convergence after 24 epochs took 8 seconds\n",
            "Accuracy: 0.7710410602588716\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.77      0.81     50895\n",
            "           1       0.66      0.77      0.71     29840\n",
            "\n",
            "    accuracy                           0.77     80735\n",
            "   macro avg       0.76      0.77      0.76     80735\n",
            "weighted avg       0.78      0.77      0.77     80735\n",
            "\n",
            "Confusion Matrix:\n",
            " [[39310 11585]\n",
            " [ 6900 22940]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "f1_scores = cross_val_score(clf, X, y, cv=cv, scoring='f1')\n",
        "print(f\"\\n5-fold CV F1 scores: {f1_scores}\")\n",
        "print(\"Mean F1 score:\", f1_scores.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gco80ryB1n0j",
        "outputId": "31535be3-a8aa-475c-bb3e-4cfe1bec2dd8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "convergence after 24 epochs took 12 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "convergence after 24 epochs took 10 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "convergence after 26 epochs took 9 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "convergence after 24 epochs took 8 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "convergence after 23 epochs took 8 seconds\n",
            "\n",
            "5-fold CV F1 scores: [0.7104657  0.71266196 0.70931538 0.71505501 0.71092562]\n",
            "Mean F1 score: 0.711684733151533\n"
          ]
        }
      ]
    }
  ]
}