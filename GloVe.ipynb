{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3ad5a08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7d2f54a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f72abb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\A\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\A\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c8284fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from Assignment 4\n",
    "PADDING_WORD = \"<PAD>\"\n",
    "UNKNOWN = \"<UNK>\"\n",
    "def load_glove_embeddings(embedding_file,\n",
    "                          padding_word=PADDING_WORD, \n",
    "                          unknown_word=UNKNOWN):\n",
    "    \"\"\"\n",
    "    Reads Glove embeddings from a file.\n",
    "\n",
    "    Returns vector dimensionality, the word_to_id mapping (as a dict),\n",
    "    and the embeddings (as a list of lists).\n",
    "    \"\"\"\n",
    "    # we just read in all the embeddings from the file and index the words in the order we read them\n",
    "    # starting from 2 bcs we have padding as 0 and unknown as 1\n",
    "    word_to_id = {}  # Dictionary to store word-to-ID mapping\n",
    "    word_to_id[padding_word] = 0\n",
    "    word_to_id[unknown_word] = 1\n",
    "    embeddings = []\n",
    "    with open(embedding_file, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            data = line.split()\n",
    "            word = data[0]\n",
    "            vec = [float(x) for x in data[1:]]\n",
    "            embeddings.append(vec)\n",
    "            word_to_id[word] = len(word_to_id)\n",
    "    D = len(embeddings[0])\n",
    "\n",
    "    embeddings.insert(word_to_id[padding_word], [0]*D)  # <PAD> has an embedding of just zeros\n",
    "    embeddings.insert(word_to_id[unknown_word], [-1]*D)      # <UNK> has an embedding of just minus-ones\n",
    "\n",
    "    return D, word_to_id, embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18eb1c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the cleaned dataset\n",
    "path = \"data\\question_pairs_cleaned.csv\"\n",
    "df = pd.read_csv(\n",
    "    path,\n",
    "    engine=\"python\",\n",
    "    quotechar='\"',\n",
    "    sep=\",\",\n",
    "    names=['id','qid1','qid2','question1','question2','is_duplicate'],\n",
    "    header=0,\n",
    "    on_bad_lines='skip'\n",
    ")\n",
    "df = df.dropna(subset=['question1','question2','is_duplicate']).copy()\n",
    "df['is_duplicate'] = df['is_duplicate'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6c1a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "D, word_to_id, embeddings = load_glove_embeddings(\"data\\glove.6B\\glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023e6293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "sentence_2_embed = [None]*(max(df[\"qid2\"])+1)\n",
    "def avg_glove_embedding(question):\n",
    "    question = question.lower()\n",
    "    tokens = nltk.word_tokenize(question)\n",
    "    #stop_words = set(stopwords.words('english'))\n",
    "    #tokens = [t for t in tokens if t not in stop_words] # remove stopwords\n",
    "    # if all tokens were stop words return the 0 vector\n",
    "    if len(tokens) == 0:\n",
    "        return np.zeros(D)\n",
    "    avg = np.zeros(D)\n",
    "    for w in tokens:\n",
    "        if w in word_to_id:\n",
    "            current_embedding = embeddings[word_to_id[w]]\n",
    "        else:\n",
    "            current_embedding = embeddings[word_to_id[UNKNOWN]] #just in case we change from the 0 vector in the future currently no effect\n",
    "        avg += current_embedding\n",
    "    avg/= len(tokens)\n",
    "    return avg\n",
    "def parse_row(row):\n",
    "    avg = [np.zeros(D),np.zeros(D)] #one for question1 one for question2\n",
    "    for q in [1,2]:\n",
    "        #check if we already did this question, if so just get the stored one\n",
    "        if sentence_2_embed[row[f\"qid{q}\"]] is not None:\n",
    "            avg[q-1] = sentence_2_embed[row[f\"qid{q}\"]]\n",
    "        else:\n",
    "            question = row[f\"question{q}\"].lower()\n",
    "            avg[q-1] = avg_glove_embedding(question)\n",
    "            sentence_2_embed[row[f\"qid{q}\"]] = avg[q-1]# save the question by its id\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807377aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoints = []\n",
    "labels = []\n",
    "#calculates the average glove embedding of both sentences\n",
    "for index, row in df.iterrows():\n",
    "    avg = parse_row(row)\n",
    "    datapoints.append([avg[0],avg[1]])\n",
    "    labels.append(row[\"is_duplicate\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ecefff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 24 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 40 epochs took 11 seconds\n",
      "Accuracy: 0.6285254226791355\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.62      0.68     50895\n",
      "           1       0.50      0.65      0.56     29840\n",
      "\n",
      "    accuracy                           0.63     80735\n",
      "   macro avg       0.62      0.63      0.62     80735\n",
      "weighted avg       0.66      0.63      0.63     80735\n",
      "\n",
      "Confusion Matrix:\n",
      " [[31453 19442]\n",
      " [10549 19291]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array(datapoints)           # (n_samples, 2, D)\n",
    "X_flat = X.reshape(X.shape[0], -1) # (n_samples, 2*D)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_flat, labels, test_size=0.2, stratify=labels, random_state=42 # stratisfy means keep the same ratio of 0/1 in both train and test\n",
    ")\n",
    "\n",
    "\n",
    "# 7. Train a LogisticRegression on the sparse matrix\n",
    "clf = LogisticRegression(\n",
    "    solver='saga',        # sparse-friendly solver\n",
    "    max_iter=500,\n",
    "    class_weight='balanced',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 8. Evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ccf214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of 1 labels: 0.6303954021991905\n"
     ]
    }
   ],
   "source": [
    "print(\"Percentage of 1 labels: {}\".format(sum(y_train)/len(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a67022a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "403672"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c9ca64",
   "metadata": {},
   "source": [
    "- vanilla `0.6285, 0.62 macro f1, 0.63 weighted f1` \n",
    "- removing stop words had basically no effect accuracy wise but increased run time by 3x `0.624 accuracy, 0.62 macro f1, 0.63 weighted f1`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799676f6",
   "metadata": {},
   "source": [
    "## demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d629d443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class 0 with prob 0.5896645266843437 and class 1 with prob 0.4103354733156564\n"
     ]
    }
   ],
   "source": [
    "q1 = input(\"Enter the first question: \")\n",
    "q2 = input(\"Enter second question: \")\n",
    "avg = [np.zeros(D),np.zeros(D)]\n",
    "for i,question in enumerate([q1,q2]):\n",
    "    avg[i] = avg_glove_embedding(question)\n",
    "avg = np.array([avg]).flatten()\n",
    "avg = np.expand_dims(avg,axis=0)\n",
    "prob = clf.predict_proba(avg)[0]\n",
    "print(f\"Predicted class 0 with prob {prob[0]} and class 1 with prob {prob[1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langeng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
