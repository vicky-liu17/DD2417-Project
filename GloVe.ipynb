{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d2f54a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8284fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from Assignment 4\n",
    "PADDING_WORD = \"<PAD>\"\n",
    "UNKNOWN = \"<UNK>\"\n",
    "def load_glove_embeddings(embedding_file,\n",
    "                          padding_word=PADDING_WORD, \n",
    "                          unknown_word=UNKNOWN):\n",
    "    \"\"\"\n",
    "    Reads Glove embeddings from a file.\n",
    "\n",
    "    Returns vector dimensionality, the word_to_id mapping (as a dict),\n",
    "    and the embeddings (as a list of lists).\n",
    "    \"\"\"\n",
    "    # we just read in all the embeddings from the file and index the words in the order we read them\n",
    "    # starting from 2 bcs we have padding as 0 and unknown as 1\n",
    "    word_to_id = {}  # Dictionary to store word-to-ID mapping\n",
    "    word_to_id[padding_word] = 0\n",
    "    word_to_id[unknown_word] = 1\n",
    "    embeddings = []\n",
    "    with open(embedding_file, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            data = line.split()\n",
    "            word = data[0]\n",
    "            vec = [float(x) for x in data[1:]]\n",
    "            embeddings.append(vec)\n",
    "            word_to_id[word] = len(word_to_id)\n",
    "    D = len(embeddings[0])\n",
    "\n",
    "    embeddings.insert(word_to_id[padding_word], [0]*D)  # <PAD> has an embedding of just zeros\n",
    "    embeddings.insert(word_to_id[unknown_word], [-1]*D)      # <UNK> has an embedding of just minus-ones\n",
    "\n",
    "    return D, word_to_id, embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18eb1c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the cleaned dataset\n",
    "path = \"data\\question_pairs_cleaned.csv\"\n",
    "df = pd.read_csv(\n",
    "    path,\n",
    "    engine=\"python\",\n",
    "    quotechar='\"',\n",
    "    sep=\",\",\n",
    "    names=['id','qid1','qid2','question1','question2','is_duplicate'],\n",
    "    header=0,\n",
    "    on_bad_lines='skip'\n",
    ")\n",
    "df = df.dropna(subset=['question1','question2','is_duplicate']).copy()\n",
    "df['is_duplicate'] = df['is_duplicate'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb6c1a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "D, word_to_id, embeddings = load_glove_embeddings(\"data\\glove.6B\\glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "023e6293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "sentence_2_embed = [None]*(max(df[\"qid2\"])+1)\n",
    "def avg_glove_embedding(question):\n",
    "    question = question.lower()\n",
    "    tokens = nltk.word_tokenize(question)\n",
    "    #stop_words = set(stopwords.words('english'))\n",
    "    #tokens = [t for t in tokens if t not in stop_words] # remove stopwords\n",
    "    # if all tokens were stop words return the 0 vector\n",
    "    if len(tokens) == 0:\n",
    "        return np.zeros(D)\n",
    "    avg = np.zeros(D)\n",
    "    for w in tokens:\n",
    "        if w in word_to_id:\n",
    "            current_embedding = embeddings[word_to_id[w]]\n",
    "        else:\n",
    "            current_embedding = embeddings[word_to_id[UNKNOWN]] #just in case we change from the 0 vector in the future currently no effect\n",
    "        avg += current_embedding\n",
    "    avg/= len(tokens)\n",
    "    return avg\n",
    "def parse_row(row):\n",
    "    avg = [np.zeros(D),np.zeros(D)] #one for question1 one for question2\n",
    "    for q in [1,2]:\n",
    "        #check if we already did this question, if so just get the stored one\n",
    "        if sentence_2_embed[row[f\"qid{q}\"]] is not None:\n",
    "            avg[q-1] = sentence_2_embed[row[f\"qid{q}\"]]\n",
    "        else:\n",
    "            question = row[f\"question{q}\"].lower()\n",
    "            avg[q-1] = avg_glove_embedding(question)\n",
    "            sentence_2_embed[row[f\"qid{q}\"]] = avg[q-1]# save the question by its id\n",
    "    #avg.append(np.dot(avg[0],avg[1]))\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "807377aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoints = []\n",
    "labels = []\n",
    "#calculates the average glove embedding of both sentences\n",
    "for index, row in df.iterrows():\n",
    "    avg = parse_row(row)\n",
    "    datapoints.append([avg[0],avg[1]])\n",
    "    labels.append(row[\"is_duplicate\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ecefff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 24 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "X = np.array(datapoints)           # (n_samples, 2, D)\n",
    "X_flat = X.reshape(X.shape[0], -1) # (n_samples, 2*D)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_flat, labels, test_size=0.2, stratify=labels, random_state=42 # stratisfy means keep the same ratio of 0/1 in both train and test\n",
    ")\n",
    "\n",
    "\n",
    "# 7. Train a LogisticRegression on the sparse matrix\n",
    "clf = LogisticRegression(\n",
    "    solver='saga',        # sparse-friendly solver\n",
    "    max_iter=500,\n",
    "    class_weight='balanced',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 8. Evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f268229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 101\n"
     ]
    }
   ],
   "source": [
    "n_weights = clf.coef_.size       \n",
    "n_biases  = clf.intercept_.size\n",
    "n_params  = n_weights + n_biases\n",
    "print(f\"Total number of parameters: {n_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda2f286",
   "metadata": {},
   "source": [
    "Output from our experiments:\n",
    "\n",
    "D = 300\n",
    "```\n",
    "Accuracy: 0.6756425342168824\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.78      0.67      0.72     50895\n",
    "           1       0.55      0.68      0.61     29840\n",
    "\n",
    "    accuracy                           0.68     80735\n",
    "   macro avg       0.67      0.68      0.67     80735\n",
    "weighted avg       0.70      0.68      0.68     80735\n",
    "\n",
    "Confusion Matrix:\n",
    " [[34322 16573]\n",
    " [ 9614 20226]]\n",
    "```\n",
    "\n",
    "D = 50\n",
    "```\n",
    "Accuracy: 0.6285254226791355\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.75      0.62      0.68     50895\n",
    "           1       0.50      0.65      0.56     29840\n",
    "\n",
    "    accuracy                           0.63     80735\n",
    "   macro avg       0.62      0.63      0.62     80735\n",
    "weighted avg       0.66      0.63      0.63     80735\n",
    "\n",
    "Confusion Matrix:\n",
    " [[31 453 19 442]\n",
    " [10 549 19 291]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ccf214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of 1 labels: 0.36960459780080945\n"
     ]
    }
   ],
   "source": [
    "print(\"Percentage of 1 labels: {}\".format(sum(y_train)/len(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799676f6",
   "metadata": {},
   "source": [
    "## demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d629d443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class 0 with prob 0.8852010131033184 and class 1 with prob 0.11479898689668157\n"
     ]
    }
   ],
   "source": [
    "q1 = input(\"Enter the first question: \")\n",
    "q2 = input(\"Enter second question: \")\n",
    "avg = [np.zeros(D),np.zeros(D)]\n",
    "for i,question in enumerate([q1,q2]):\n",
    "    avg[i] = avg_glove_embedding(question)\n",
    "avg = np.array([avg]).flatten()\n",
    "avg = np.expand_dims(avg,axis=0)\n",
    "prob = clf.predict_proba(avg)[0]\n",
    "print(f\"Predicted class 0 with prob {prob[0]} and class 1 with prob {prob[1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langeng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
